{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys as sys\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "\n",
    "import ds_manipulation as dsm\n",
    "#import w2vec as w2v\n",
    "import test_f as tst\n",
    "import LSTM as lstm\n",
    "import LSTMParameters as LSTMParam\n",
    "\n",
    "import argparse\n",
    "\n",
    "df_train_inner = pd.read_csv('./train_inner_10.csv')\n",
    "df_train_inner = dsm.remove_single_actions(df_train_inner)\n",
    "df_train_inner =  dsm.remove_nonitem_actions(df_train_inner)\n",
    "\n",
    "df_test_inner = pd.read_csv('./test_inner_10.csv')\n",
    "df_test_inner = dsm.remove_single_actions(df_test_inner)\n",
    "df_test_inner = dsm.remove_nonitem_actions(df_test_inner)\n",
    "\n",
    "df_gt_inner = pd.read_csv('./gt_inner_10.csv')\n",
    "\n",
    "df_test_inner, df_gt_inner = dsm.remove_test_single_actions(df_test_inner, df_gt_inner)\n",
    "\n",
    "df_test_dev = pd.read_csv('./test_10.csv')\n",
    "df_test_dev = dsm.remove_single_actions(df_test_dev)\n",
    "df_test_dev = dsm.remove_nonitem_actions(df_test_dev)\n",
    "\n",
    "df_gt_dev = pd.read_csv('./gt_10.csv')\n",
    "\n",
    "df_test_inner, df_gt_inner = dsm.remove_test_single_actions(df_test_dev, df_gt_dev)\n",
    "\n",
    "\n",
    "df_corpus = pd.concat([df_train_inner, df_test_inner, df_test_dev])\n",
    "\n",
    "print(df_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dsm.get_corpus(df_corpus)\n",
    "\n",
    "'''\n",
    "STEP 2: ENCODING TO CREATE DICTIONARY\n",
    "'''\n",
    "\n",
    "#w2vec item encoding\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(corpus, min_count=1, window=param.window, sg=1)\n",
    "\n",
    "n_features = len(word2vec.wv['666856'])\n",
    "\n",
    "#hotel_dict = w2v.normalize_word2vec(word2vec.wv)\n",
    "\n",
    "hotel_dict = word2vec.wv\n",
    "\n",
    "#extracting metadata features\n",
    "meta_list = []\n",
    "meta_dict = []\n",
    "if param.ismeta:\n",
    "    meta_list = dsm.extract_unique_meta(df_meta)\n",
    "    meta_dict = dsm.get_meta_dict(df_meta, hotel_dict.index2word, meta_list)\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 3: PREPARE NET INPUT\n",
    "'''\n",
    "\n",
    "#this splits the training set sessions into multiple mini-sessions\n",
    "if param.batchsize == 0:\n",
    "    sessions, categories, hotels_window = dsm.prepare_input(df_train_inner)\n",
    "else:\n",
    "    sessions, categories, hotels_window = dsm.prepare_input_batched(df_train_inner, param.batchsize)\n",
    "\n",
    "test_sessions, test_hotels_window, test_clickout_index = tst.prepare_test(df_test_inner, df_gt_inner)\n",
    "\n",
    "#getting maximum window size\n",
    "max_window = 0\n",
    "if param.isimpression:\n",
    "    for window in hotels_window:\n",
    "        if len(window) > max_window:\n",
    "            max_window = len(window)\n",
    "    #if param.train == './train_1.csv':\n",
    "    max_window = 25\n",
    "\n",
    "#Setting up feature numbers\n",
    "n_hotels = len(hotel_dict.index2word)\n",
    "n_features_w2vec = len(word2vec.wv['666856'])\n",
    "n_features_meta = len(meta_list)\n",
    "n_features_impression = max_window\n",
    "n_features = n_features_w2vec + n_features_meta + n_features_impression\n",
    "\n",
    "print('n_hotels is ' + str(n_hotels))\n",
    "print('n_features_w2vec is ' + str(n_features_w2vec))\n",
    "print('n_features_meta is ' + str(n_features_meta))\n",
    "print('n_features_impression is ' + str(n_features_impression))\n",
    "print('n_features is ' + str(n_features))\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 4: CREATE NETWORK\n",
    "'''\n",
    "\n",
    "#DEFINE PARAMETERS\n",
    "input_dim = n_features\n",
    "output_dim = n_hotels\n",
    "#hidden_dim = int(1/100 * (input_dim + output_dim))\n",
    "hidden_dim = param.hiddendim\n",
    "print('The model is:')\n",
    "print('input_dim is:' + str(input_dim))\n",
    "print('hidden_dim is: ' + str(hidden_dim))\n",
    "print('output_dim is:' + str(output_dim))\n",
    "layer_dim = 1\n",
    "\n",
    "#NET CREATION\n",
    "model = lstm.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim, param.iscuda)\n",
    "\n",
    "if param.iscuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# WEIGHT INIT LSTM\n",
    "#model.lstm.weight_hh_l0.data.fill_(0)\n",
    "#x = 1\n",
    "#nn.init.uniform_(model.fc.weight, -x, x)\n",
    "#nn.init.uniform_(model.fc.bias, -x, x)\n",
    "\n",
    "# WEIGHT INIT GRU???\n",
    "# TODO\n",
    "\n",
    "'''\n",
    "STEP 5: LEARNING PHASE\n",
    "'''\n",
    "\n",
    "#LOSS FUNCTION\n",
    "#loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "if param.iscuda:\n",
    "    loss_fn = loss_fn.cuda()\n",
    "\n",
    "#OPTIMIZER\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=param.learnrate)\n",
    "model.optimizer = optimizer\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "num_epochs = param.epochs\n",
    "plot_every = 1\n",
    "\n",
    "n_iters = len(sessions) * num_epochs\n",
    "print_every = 1000\n",
    "\n",
    "# Keep track of losses and acc for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "all_acc = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Training results for xgboost\n",
    "training_results_hotels = {}\n",
    "training_results_scores = {}\n",
    "\n",
    "with open('rnn_train_sub_xgb_inner.csv', mode='w') as rnn_train_sub_xgb:\n",
    "    file_writer = csv.writer(rnn_train_sub_xgb)\n",
    "    file_writer.writerow(['session_id', 'hotel_id', 'score'])\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        #model.train()\n",
    "        iter = 0\n",
    "        \n",
    "        count_correct = 0\n",
    "        count_correct_windowed = 0\n",
    "\n",
    "        print(str(len(sessions) * param.batchsize) + ' sessions to be computed')\n",
    "        \n",
    "        for index, session in enumerate(sessions):\n",
    "            iter = iter + 1\n",
    "\n",
    "            if param.batchsize == 0:\n",
    "                session_tensor = lstm.session_to_tensor(session, hotel_dict, n_features, hotels_window, max_window, meta_dict, meta_list)\n",
    "                category = categories[index]\n",
    "                category_tensor = lstm.hotel_to_category(category, hotel_dict, n_hotels)\n",
    "            else:\n",
    "                max_session_len = 0\n",
    "                for si, single_session in enumerate(session):\n",
    "                    if len(single_session) > max_session_len:\n",
    "                        max_session_len = len(single_session)\n",
    "                \n",
    "                session_tensor = lstm.sessions_to_batch(session, hotel_dict, max_session_len, n_features, hotels_window, max_window, meta_dict, meta_list)\n",
    "                category = categories[index]\n",
    "                hotel_window = hotels_window[index]\n",
    "                category_tensor = lstm.hotels_to_category_batch(category, hotel_dict, n_hotels)\n",
    "\n",
    "            \n",
    "            output, loss = lstm.train(model, loss_fn, optimizer, category_tensor, session_tensor, param.iscuda)\n",
    "\n",
    "            current_loss += loss\n",
    "            \n",
    "            guess, guess_i = lstm.category_from_output(output, hotel_dict)\n",
    "            guess_windowed_list, guess_windowed_scores_list = lstm.categories_from_output_windowed_opt(output, hotel_window, hotel_dict, pickfirst = False)\n",
    "        \n",
    "            for batch_i, category_v in enumerate(category):\n",
    "                if guess[batch_i] == category_v:\n",
    "                    count_correct = count_correct + 1\n",
    "\n",
    "                if iter % print_every == 0:\n",
    "                    print('Non-Windowed results:')\n",
    "                    correct = '✓' if guess[batch_i] == category_v else '✗ (%s)' % category_v\n",
    "                    print('(%s) %.4f %s / %s %s' % (timeSince(start), loss, session[batch_i][0]['session_id'], guess[batch_i], correct))\n",
    "\n",
    "                if guess_windowed_list[batch_i][0] == category_v:\n",
    "                    count_correct_windowed = count_correct_windowed + 1\n",
    "\n",
    "                if iter % print_every == 0:\n",
    "                    print('Windowed results:')\n",
    "                    correct = '✓' if guess_windowed_list[batch_i][0] == category_v else '✗ (%s)' % category_v\n",
    "                    print('(%s) %.4f %s / %s %s' % (timeSince(start), loss, session[batch_i][0]['session_id'], guess_windowed_list[batch_i][0], correct))\n",
    "\n",
    "                if epoch == num_epochs:   \n",
    "                    for hotel_i, hotel in enumerate(guess_windowed_list[batch_i]):\n",
    "                        # Write single hotel score\n",
    "                        file_writer.writerow([str(session[batch_i][0]['session_id']), str(hotel), str(guess_windowed_scores_list[batch_i][hotel_i])]) \n",
    "                    \n",
    "                \n",
    "        # Add current loss avg to list of losses\n",
    "        if epoch % plot_every == 0:\n",
    "            all_losses.append(current_loss / (plot_every * len(sessions)))\n",
    "            print('Epoch: ' + str(epoch) + ' Loss: ' + str(current_loss / (plot_every * len(sessions))))\n",
    "            print('%d %d%% (%s)' % (epoch, epoch / num_epochs * 100, timeSince(start)))\n",
    "            print('Found ' + str(count_correct) + ' correct clickouts among ' + str(len(sessions) * param.batchsize) + ' sessions.')\n",
    "            print('Windowed - Found ' + str(count_correct_windowed) + ' correct clickouts among ' + str(len(sessions) * param.batchsize) + ' sessions.')\n",
    "            acc = tst.test_accuracy_optimized(model, df_test_inner, df_gt_inner, test_sessions, test_hotels_window, test_clickout_index, hotel_dict, n_features, max_window, meta_dict, meta_list)\n",
    "            print(\"Score: \" + str(acc))\n",
    "            all_acc.append(acc)\n",
    "            current_loss = 0\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: PLOTTING RESULTS\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(all_losses)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(all_acc)\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 7: PREPARE TEST SET\n",
    "'''\n",
    "\n",
    "#mrr = tst.test_accuracy(model, df_test, df_gt, hotel_dict, n_features, max_window, meta_dict, meta_list, param.subname, isprint=True)\n",
    "mrr = tst.test_accuracy_optimized_classification(model, df_test_inner, df_gt_inner, test_sessions, test_hotels_window, test_clickout_index, hotel_dict, n_features, max_window, meta_dict, meta_list, param.subname, isprint=True, dev = False)\n",
    "print(\"Final score for inner: \" + str(mrr))\n",
    "\n",
    "test_sessions, test_hotels_window, test_clickout_index = tst.prepare_test(df_test_dev, df_gt_dev)\n",
    "\n",
    "mrr = tst.test_accuracy_optimized_classification(model, df_test_dev, df_gt_dev, test_sessions, test_hotels_window, test_clickout_index, hotel_dict, n_features, max_window, meta_dict, meta_list, param.subname, isprint=True, dev = True)\n",
    "print(\"Final score for dev: \" + str(mrr))\n",
    "\n",
    "'''\n",
    "STEP 8: SAVING SUBMISSION\n",
    "'''\n",
    "\n",
    "#Computing score\n",
    "#print(\"End execution with score \" + str(mrr))\n",
    "file_exists = os.path.isfile('classification_scores.csv')\n",
    "with open('classification_scores.csv', mode='a') as score_file:\n",
    "    file_writer = csv.writer(score_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    if not file_exists: # Write headers\n",
    "        file_writer.writerow(['Train set', 'Using impressions', 'Using meta', 'Hidden dimension', 'Dropout layer', '#Epochs', '#Components', 'W2Vec window', 'Learn Rate', 'batchsize', 'Score'])\n",
    "    file_writer.writerow([str(param.train), str(param.isimpression), str(param.ismeta), str(param.hiddendim), str(param.isdrop), str(param.epochs), str(param.ncomponents), str(param.window), str(param.learnrate), str(param.batchsize), str(mrr)])\n",
    "#f.send_telegram_message(\"End execution with score \" + str(mrr))\n",
    "\n",
    "#Saving loss\n",
    "with open(param.subname + '_loss.csv', mode='w') as loss_file:\n",
    "    file_writer = csv.writer(loss_file)\n",
    "    file_writer.writerow(['#Epochs'])\n",
    "    for loss in all_losses:\n",
    "        file_writer.writerow([loss])\n",
    "\n",
    "#Saving acc\n",
    "with open(param.subname + '_acc.csv', mode='w') as acc_file:\n",
    "    file_writer = csv.writer(acc_file)\n",
    "    file_writer.writerow(['#Epochs'])\n",
    "    for acc in all_acc:\n",
    "        file_writer.writerow([acc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
